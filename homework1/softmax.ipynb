{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "import utils as ut\n",
    "import evaluate as eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax():\n",
    "    def __init__(self, kwargs):\n",
    "        self.model_name = 'SoftMax'\n",
    "        self.W = torch.randn(kwargs['class_num'], kwargs['feature_dim']+1,)\n",
    "        self.N = kwargs['class_num']\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        torch.nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def train(self, X, Y, alpha=0.01, reg=2.5e4, vec=True):\n",
    "        \"\"\"\n",
    "        功能: 完成训练过程，包括(1)求解损失, 计算梯度. (2) 正则化，计算梯度，(3)更新参数 \n",
    "        输入:\n",
    "            X(Tensor):(N, K:3*32*32+1)\n",
    "            Y(Tensor):(N)\n",
    "            alpha(float):                   # 学习率\n",
    "            reg(float):                     # 正则化系数\n",
    "        输出:\n",
    "            L(int):(1)                      # 损失，范围给主程序以打印显示        \n",
    "        \"\"\"\n",
    "        \n",
    "        # 计算梯度与正则化\n",
    "        if vec:\n",
    "            L, dW = self.cal_dw_with_vec(X, Y, reg)\n",
    "        else:\n",
    "            L, dW = self.cal_dw_with_loop(X, Y, reg)\n",
    "            \n",
    "        # 更新参数\n",
    "        self.W -= alpha * dW\n",
    "        return L\n",
    "    \n",
    "    def cal_dw_with_loop(self, X, Y, reg):\n",
    "        \"\"\"\n",
    "        功能： 计算损失和梯度\n",
    "        输入:\n",
    "            X(Tensor):(K:3*32*32+1, N)\n",
    "            Y(Tensor):(C, N)\n",
    "            reg(float):                    # 正则化系数\n",
    "        输出:\n",
    "            L(int): 1                      # 损失               \n",
    "            dW(Tensor):(C,K)             # 参数梯度       \n",
    "        \"\"\"\n",
    "        L = 0.0\n",
    "        N = X.size(1)\n",
    "        K, C = self.W.size()\n",
    "        dW = torch.zeros(K, C)\n",
    "        \n",
    "        # (1) 求解损失\n",
    "        for i in range(N):\n",
    "            x = X[:,i].unsqueeze(1)            # (K,1)\n",
    "            y = Y[:,i].unsqueeze(1)            # (C,1)\n",
    "            L += -y.t().matmul(self.W).matmul(x).item() + torch.log(torch.sum(torch.exp(self.W.matmul(x)))).item()\n",
    "            dW = dW + (-y + torch.softmax(self.W.matmul(x), 0)) * x.t()\n",
    "        \n",
    "        # (2) 正则化\n",
    "        L = L / N +  0.5*reg*torch.sum(torch.pow(self.W, 2)).item()\n",
    "        dW = dW / N +  reg*self.W\n",
    "        \n",
    "        return L, dW\n",
    "    \n",
    "    def cal_dw_with_vec(self, X, Y, reg):\n",
    "        \"\"\"\n",
    "        功能： 计算损失和梯度\n",
    "        输入:\n",
    "            X(Tensor):(K:3*32*32+1, N)\n",
    "            Y(Tensor):(C, N)\n",
    "            reg(float):                    # 正则化系数\n",
    "        输出:\n",
    "            L(int): 1                      # 损失               \n",
    "            dW(Tensor):(K,C)             # 参数梯度      \n",
    "        \"\"\"\n",
    "        \n",
    "        N = X.size(1)\n",
    "        K, C = self.W.size()\n",
    "        \n",
    "        L1 = -Y.t().matmul(self.W).matmul(X)  # (N, N) \n",
    "        L2 = torch.sum(torch.exp(self.W.matmul(X)), 0)     # (C, N)\n",
    "        L = torch.sum(L1[range(N), range(N)]).item() + torch.sum(torch.log(L2)).item()\n",
    "        dW = -Y.matmul(X.t()) + torch.softmax(self.W.matmul(X), 0).matmul(X.t())\n",
    "        \n",
    "        L = L / N +  0.5*reg*torch.sum(torch.pow(self.W, 2)).item()\n",
    "        dW = dW / N + reg*self.W\n",
    "        return L, dW\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        功能: 预测输入数据标签\n",
    "        输入:\n",
    "            X(Tensor): (K, N)\n",
    "        输出:\n",
    "            labels(Tensor): (N)\n",
    "        \"\"\"\n",
    "        S = X.t().matmul(self.W.t())  # (N, C)\n",
    "        return torch.max(S, 1)[1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = ut.data_load('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 200\n",
    "opt = {\"feature_dim\":3*32*32, \"class_num\":10}\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_num, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_num, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(alpha, reg, epoches):\n",
    "    \"\"\"\n",
    "    功能：完成训练过程\n",
    "    输入:\n",
    "        alpha(int):(1)     # 学习率\n",
    "        reg(int):(1)       # 正则化系数\n",
    "        epoches(int):(1)   # 迭代次数\n",
    "    输出:\n",
    "        softMaxEr(class) 训练好的模型\n",
    "        alpha\n",
    "        reg\n",
    "    \"\"\"\n",
    "    softMaxEr = SoftMax(opt)\n",
    "    for epoch in range(epoches):\n",
    "        train_data_interator = enumerate(train_loader)\n",
    "        train_steps = test_steps = (len(train_set) + batch_num - 1) // batch_num\n",
    "\n",
    "        t = trange(train_steps)\n",
    "        loss_avg = ut.RunningAverage()\n",
    "        print(\"epoch:{}\".format(epoch))\n",
    "        for i in t:\n",
    "            idx, data = next(train_data_interator)\n",
    "            X_batch, Y = data\n",
    "            X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "            X_batch = torch.cat((torch.ones(X_batch.size(0),1), X_batch), 1)   \n",
    "            X_batch = X_batch.t()\n",
    "            C, N = 10, X_batch.size(1)\n",
    "            Y_batch = torch.zeros(C, N)\n",
    "            Y_batch[Y.tolist(), range(N)] = 1\n",
    "            \n",
    "            loss = softMaxEr.train(X_batch, Y_batch, alpha=1e-4, reg=1, vec=True)\n",
    "            loss_avg.update(loss)\n",
    "            t.set_postfix(loss='{:05.3f}/{:05.3f}'.format(loss_avg(), loss))\n",
    "        print(loss_avg())\n",
    "    return softMaxEr\n",
    "\n",
    "def evaluate(model):\n",
    "    \"\"\"\n",
    "    功能：使用训练好的模型进行预测，并评测结果\n",
    "    输入: \n",
    "        svmEr(class) 训练好的模型\n",
    "    输出: \n",
    "        acc(int):(1) 模型准确率\n",
    "    \"\"\"\n",
    "    test_data_interator = enumerate(test_loader)\n",
    "    test_steps = test_steps = (len(test_set) + batch_num - 1) // batch_num\n",
    "\n",
    "    t = trange(test_steps)\n",
    "    Y_predict = []\n",
    "    Y_true = []\n",
    "    for i in t:\n",
    "        idx, data = next(test_data_interator)\n",
    "        X_batch, Y_batch = data\n",
    "        Y_true.extend(Y_batch.tolist())\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "        X_batch = torch.cat((torch.ones(X_batch.size(0),1), X_batch), 1)   \n",
    "        \n",
    "        X_batch = X_batch.t()\n",
    "\n",
    "        y = model.predict(X_batch)\n",
    "        Y_predict.extend(y.tolist())\n",
    "        \n",
    "    Y_predict = torch.LongTensor(Y_predict)\n",
    "    Y_true = torch.LongTensor(Y_true)\n",
    "    acc = torch.sum(Y_predict == Y_true).item() /len(Y_predict)\n",
    "    \n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strs = [0, 1, 10, 100, 1000]\n",
    "\n",
    "result = {}\n",
    "\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "\n",
    "for lr in lrs:\n",
    "    for reg in reg_strs:\n",
    "        model = train(lr, reg, 100)\n",
    "        acc = evaluate(model)\n",
    "        print(\"lr:{}; reg:{}; acc:{}\".format(lr, reg, acc))\n",
    "        if acc > best_acc:\n",
    "            best_lr = lr\n",
    "            best_reg = reg\n",
    "            best_model = model\n",
    "        result[(lr, reg)] = acc\n",
    "print(\"the best: lr:{}; reg:{}; acc:{}\".format(best_lr, best_reg, best_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
