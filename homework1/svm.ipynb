{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "import utils as ut\n",
    "import evaluate as eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    def __init__(self, kwargs):\n",
    "        self.model_name = 'SVM'\n",
    "        self.W = torch.randn(kwargs['feature_dim']+1, kwargs['class_num'])\n",
    "        self.N = kwargs['class_num']\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        torch.nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def train(self, X, Y, alpha=0.01, reg=2.5e4, vec=True):\n",
    "        \"\"\"\n",
    "        功能: 完成训练过程，包括(1)求解损失, 计算梯度. (2) 正则化，计算梯度，(3)更新参数 \n",
    "        输入:\n",
    "            X(Tensor):(N, K:3*32*32+1)\n",
    "            Y(Tensor):(N)\n",
    "            alpha(float):                   # 学习率\n",
    "            reg(float):                     # 正则化系数\n",
    "        输出:\n",
    "            L(int):(1)                      # 损失，范围给主程序以打印显示        \n",
    "        \"\"\"\n",
    "        \n",
    "        # 计算梯度与正则化\n",
    "        if vec:\n",
    "            L, dW = self.cal_dw_with_vec(X, Y, reg)\n",
    "        else:\n",
    "            L, dW = self.cal_dw_with_loop(X, Y, reg)\n",
    "            \n",
    "        # 更新参数\n",
    "        self.W -= alpha * dW\n",
    "        return L\n",
    "    \n",
    "    def cal_dw_with_loop(self, X, Y, reg):\n",
    "        \"\"\"\n",
    "        功能： 计算损失和梯度\n",
    "        输入:\n",
    "            X(Tensor):(N, K:3*32*32+1)\n",
    "            Y(Tensor):(N)\n",
    "            reg(float):                    # 正则化系数\n",
    "        输出:\n",
    "            L(int): 1                      # 损失               \n",
    "            dW(Tensor):(K+1,C)             # 参数梯度       \n",
    "        \"\"\"\n",
    "        L = 0.0\n",
    "        N = X.size(0)\n",
    "        F, C = self.W.size()\n",
    "        dW = torch.zeros(F, C)\n",
    "        \n",
    "        # (1) 求解损失\n",
    "        for idx, Xi in enumerate(X):\n",
    "            yi = Y[idx]\n",
    "            scores = Xi.matmul(self.W)\n",
    "            syi = scores[yi]\n",
    "            for j in range(self.N):\n",
    "                if j == yi:\n",
    "                    continue\n",
    "                sj = scores[j]\n",
    "                if syi - sj - 1 < 0:\n",
    "                    L += (sj - syi + 1).item()\n",
    "                    dW[:,j] += Xi.t()\n",
    "                    dW[:,yi] -= Xi.t()\n",
    "        \n",
    "        # (2) 正则化\n",
    "        L = L / N +  0.5*reg*torch.sum(torch.pow(self.W, 2)).item()\n",
    "        dW = dW / N +  reg*self.W\n",
    "        \n",
    "        return L, dW\n",
    "    \n",
    "    def cal_dw_with_vec(self, X, Y, reg):\n",
    "        \"\"\"\n",
    "        功能： 计算损失和梯度\n",
    "        输入:\n",
    "            X(Tensor):(N, K:3*32*32+1)\n",
    "            Y(Tensor):(N)\n",
    "            reg(float):                    # 正则化系数\n",
    "        输出:\n",
    "            L(int): 1                      # 损失               \n",
    "            dW(Tensor):(K+1,C)             # 参数梯度       \n",
    "        \"\"\"\n",
    "        \n",
    "        N = X.size(0)\n",
    "        F, C = self.W.size()\n",
    "        \n",
    "        score = X.matmul(self.W)                                       # (N, C)\n",
    "        correct = score[range(N), Y.tolist()].unsqueeze(1)             # (N, 1)\n",
    "        score = torch.relu(score-correct+1)                            # (N, C)\n",
    "        score[range(N), Y.tolist()] = 0\n",
    "        \n",
    "        L = torch.sum(score).item()\n",
    "        L = L / N +  0.5*reg*torch.sum(torch.pow(self.W, 2)).item()\n",
    "        \n",
    "        \n",
    "        dW = torch.zeros(F, C)\n",
    "        mask = torch.zeros(N, C)\n",
    "        mask[score>0] = 1                                              # (N,C)\n",
    "        mask[range(N), Y.tolist()] = -torch.sum(mask, 1)               # (N,C)\n",
    "        dW = X.t().matmul(mask)                                        # (F,C)\n",
    "\n",
    "        dW = dW / N + reg*self.W\n",
    "        return L, dW\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        功能: 预测输入数据标签\n",
    "        输入:\n",
    "            X(Tensor): (N, 3*32*32)\n",
    "        输出:\n",
    "            labels(Tensor): (N)\n",
    "        \"\"\"\n",
    "        S = X.matmul(self.W)  # (N, C)\n",
    "        return torch.max(S, 1)[1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = ut.data_load('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 200\n",
    "opt = {\"feature_dim\":3*32*32, \"class_num\":10}\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_num, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_num, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(alpha, reg, epoches):\n",
    "    \"\"\"\n",
    "    功能：完成训练过程\n",
    "    输入:\n",
    "        alpha(int):(1)     # 学习率\n",
    "        reg(int):(1)       # 正则化系数\n",
    "        epoches(int):(1)   # 迭代次数\n",
    "    输出:\n",
    "        svmEr(class) 训练好的模型\n",
    "        alpha\n",
    "        reg\n",
    "    \"\"\"\n",
    "    svmEr = SVM(opt)\n",
    "    for epoch in range(epoches):\n",
    "        train_data_interator = enumerate(train_loader)\n",
    "        train_steps = test_steps = (len(train_set) + batch_num - 1) // batch_num\n",
    "\n",
    "        t = trange(train_steps)\n",
    "        loss_avg = ut.RunningAverage()\n",
    "        print(\"epoch:{}\".format(epoch))\n",
    "        for i in t:\n",
    "            idx, data = next(train_data_interator)\n",
    "            X_batch, Y_batch = data\n",
    "            X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "            X_batch = torch.cat((torch.ones(X_batch.size(0),1), X_batch), 1)   \n",
    "\n",
    "            loss = svmEr.train(X_batch, Y_batch, alpha=1e-4, reg=1, vec=True)\n",
    "            loss_avg.update(loss)\n",
    "            t.set_postfix(loss='{:05.3f}/{:05.3f}'.format(loss_avg(), loss))\n",
    "        print(loss_avg())\n",
    "    return svmEr\n",
    "\n",
    "def evaluate(svmEr):\n",
    "    \"\"\"\n",
    "    功能：使用训练好的模型进行预测，并评测结果\n",
    "    输入: \n",
    "        svmEr(class) 训练好的模型\n",
    "    输出: \n",
    "        acc(int):(1) 模型准确率\n",
    "    \"\"\"\n",
    "    test_data_interator = enumerate(test_loader)\n",
    "    test_steps = test_steps = (len(test_set) + batch_num - 1) // batch_num\n",
    "\n",
    "    t = trange(test_steps)\n",
    "    Y_predict = []\n",
    "    Y_true = []\n",
    "    for i in t:\n",
    "        idx, data = next(test_data_interator)\n",
    "        X_batch, Y_batch = data\n",
    "        Y_true.extend(Y_batch.tolist())\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)\n",
    "        X_batch = torch.cat((torch.ones(X_batch.size(0),1), X_batch), 1)   \n",
    "\n",
    "        y = svmEr.predict(X_batch)\n",
    "        Y_predict.extend(y.tolist())\n",
    "        \n",
    "    Y_predict = torch.LongTensor(Y_predict)\n",
    "    Y_true = torch.LongTensor(Y_true)\n",
    "    acc = torch.sum(Y_predict == Y_true).item() /len(Y_predict)\n",
    "    \n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 7/250 [00:00<00:03, 67.91it/s, loss=19.389/19.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:01<00:00, 129.25it/s, loss=18.016/17.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.01562213958741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/250 [00:00<00:02, 97.63it/s, loss=17.022/16.814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:02<00:00, 123.84it/s, loss=16.391/15.977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.390617867126466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 13/250 [00:00<00:02, 115.95it/s, loss=15.721/15.361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:02<00:00, 121.91it/s, loss=15.407/15.120]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.40744403762817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 11/250 [00:00<00:02, 106.64it/s, loss=14.958/14.689]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:02<00:00, 121.24it/s, loss=14.691/14.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.69140471282959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/250 [00:00<00:02, 99.67it/s, loss=14.353/14.064]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 73/250 [00:00<00:01, 117.73it/s, loss=14.260/14.209]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-97faab5c9807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreg_strs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msvmEr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvmEr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lr:{}; reg:{}; acc:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5d5cacb0fb51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(alpha, reg, epoches)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_interator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 73/250 [00:19<00:01, 117.73it/s, loss=14.260/14.209]"
     ]
    }
   ],
   "source": [
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strs = [0, 1, 10, 100, 1000]\n",
    "\n",
    "result = {}\n",
    "\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_svm = None\n",
    "best_acc = -1\n",
    "\n",
    "for lr in lrs:\n",
    "    for reg in reg_strs:\n",
    "        svmEr = train(lr, reg, 25)\n",
    "        acc = evaluate(svmEr)\n",
    "        print(\"lr:{}; reg:{}; acc:{}\".format(lr, reg, acc))\n",
    "        if acc > best_acc:\n",
    "            best_lr = lr\n",
    "            best_reg = reg\n",
    "            best_svm = svmEr\n",
    "        result[(lr, reg)] = acc\n",
    "print(\"the best: lr:{}; reg:{}; acc:{}\".format(best_lr, best_reg, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
